{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from os import walk\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import time\n",
    "import datetime\n",
    "import io\n",
    "import math\n",
    "from collections import Counter\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_dir = r'C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\stop_small'\n",
    "t_dir = r'C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\small_TFIDF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_txt_files = []\n",
    "for file in Path(r'C:\\\\Users\\\\Vrindavan\\\\Downloads\\\\Krithika_Verma_HW1\\\\stop_small').rglob(\"*.csv\"):\n",
    "    all_txt_files.append(file.parent / file.name)\n",
    "\n",
    "\n",
    "# creating dictionary to store the number of documents containing the word\n",
    "DF = Counter()\n",
    "for i in all_txt_files:\n",
    "    \n",
    "    reader = pd.read_csv(i)\n",
    "    terms = tuple(reader.term)\n",
    "    for tokens in terms:\n",
    "        if tokens not in DF.keys():\n",
    "            DF[tokens] = 1\n",
    "        else:\n",
    "            DF[tokens] += 1\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "N = len(all_txt_files)\n",
    "idf = Counter()\n",
    "import math\n",
    "\n",
    "# calculating IDF and storing the results of all words in the corpus in idf dictionary    \n",
    "for ( term, term_freq) in DF.items():\n",
    "    idf[term]= np.log(float(N) / (term_freq+1))\n",
    "\n",
    "\n",
    "# Multiplying TF and IDF\n",
    "counter1 = 1\n",
    "dk = {}\n",
    "tokens_files2 = os.listdir(k_dir)\n",
    "for entry5 in tokens_files2:\n",
    "    path5 = os.path.join(k_dir,entry5)    \n",
    "    df55 = pd.read_csv(path5)\n",
    "    tf_idf = {}\n",
    "    tf_dict = dict(zip(df55.term, df55.tf))\n",
    "    for key, val in tf_dict.items():\n",
    "        tf_idf[key] = float(val) * idf[key]\n",
    "        \n",
    "        \n",
    "        \n",
    "    file_name5 , file_extension5 = os.path.splitext(entry5)\n",
    "    output_path5 = os.path.join(t_dir, file_name5 +'.csv')\n",
    "    with open (output_path5, 'w', encoding = 'utf-8', newline = '', errors= 'ignore') as outfile5:\n",
    "        writer1 = csv.writer(outfile5)\n",
    "        for key2,val2 in tf_idf.items(): #key= lambda item: (item[1],item[0]), reverse = True):\n",
    "            writer1.writerow([str(key2),str(counter1),str(tf_idf[key2])])\n",
    "        \n",
    "    counter1 += 1 # storing the DocId in each file \n",
    "    df2 = pd.read_csv(output_path5, header = None)\n",
    "    df2.rename(columns={0 : 'Term', 1:'DocId', 2: 'TfIdf'}, inplace = True)\n",
    "    df2.to_csv(output_path5,index = False)\n",
    "    \n",
    "    \n",
    "tokens_files3 = os.listdir(t_dir)\n",
    "df4 = []\n",
    "for entry6 in tokens_files3:\n",
    "    path6 = os.path.join(t_dir,entry6)\n",
    "    \n",
    "    df = pd.read_csv(path6, delimiter = ',')\n",
    "    df4.append(df)\n",
    "    \n",
    "con = pd.concat(df4, axis=0)  \n",
    "con.to_csv('mat.csv',index =None)\n",
    "\n",
    "df2 = pd.read_csv('mat.csv',delimiter = ',')\n",
    "df3 = df2.pivot_table(values = 'TfIdf',index= 'Term',columns='DocId')\n",
    "df5 = df3.fillna(value= 0, method= None, axis= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-6ef67cbd359f>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-6ef67cbd359f>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    for j+1 in all_txt_files\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "all_txt_files = []\n",
    "for file in Path(r'C:\\\\Users\\\\Vrindavan\\\\Downloads\\\\Krithika_Verma_HW1\\\\stop_small').rglob(\"*.csv\"):\n",
    "    all_txt_files.append(file.parent / file.name)\n",
    "\n",
    "\n",
    "# creating dictionary to store the number of documents containing the word\n",
    "def freq_count(file):\n",
    "    DF = Counter()\n",
    "    for i in all_txt_files:\n",
    "    \n",
    "        reader = pd.read_csv(i)\n",
    "        terms = tuple(reader.term)\n",
    "        for tokens in terms:\n",
    "            if tokens not in DF.keys():\n",
    "                DF[tokens] = 1\n",
    "            else:\n",
    "                DF[tokens] += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-982e5e32c565>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_txt_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_txt_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdocumentsim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-982e5e32c565>\u001b[0m in \u001b[0;36mdocumentsim\u001b[1;34m(i, j)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdocumentsim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mword_list1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mword_list2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_list2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'freq_count' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = []\n",
    "def vector(d1,d2):\n",
    "    nr = dotprod(d1,d2)\n",
    "    dr = math.sqrt(dotprod(d1,d1) * dotprod(d2,d2))\n",
    "    return math.acos(nr/dr)\n",
    "\n",
    "def documentsim(i,j):\n",
    "    word_list1 = freq_count(i)\n",
    "    word_list2 = freq_count(j)\n",
    "    distance = vector(word_list1, word_list2)\n",
    "    \n",
    "    df1.append(distance)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "for i in all_txt_files:\n",
    "    for j in all_txt_files:\n",
    "        documentsim(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-dbb6926a2e04>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-dbb6926a2e04>\"\u001b[1;36m, line \u001b[1;32m14\u001b[0m\n\u001b[1;33m    if tokens\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "all_txt_files = []\n",
    "for file in Path(r'C:\\\\Users\\\\Vrindavan\\\\Downloads\\\\Krithika_Verma_HW1\\\\small_TFIDF').rglob(\"*.csv\"):\n",
    "    all_txt_files.append(file.parent / file.name)\n",
    "\n",
    "\n",
    "for i in all_txt_files:\n",
    "    for j in all_txt_files:\n",
    "        df1 = pd.read_csv(i)\n",
    "        df2 = pd.read_csv(j)\n",
    "        terms1 = tuple(reader.Term)\n",
    "        terms2 = tuple(reader.Term)\n",
    "        for tokens in terms1:\n",
    "            for tokens2 in terms2:\n",
    "                if tokens\n",
    "            print(tokens)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\tokenized\\001.txt :\n",
      "8234 lines, \n",
      "1308 words, \n",
      "585 distinct words\n",
      "File C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\tokenized\\001.txt :\n",
      "8234 lines, \n",
      "1308 words, \n",
      "585 distinct words\n",
      "The distance between the documents is:  0.000000 (radians)\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import string \n",
    "import sys \n",
    "\n",
    "# reading the text file \n",
    "# This functio will return a \n",
    "# list of the lines of text \n",
    "# in the file.\n",
    "\n",
    "d1 = r'C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\tokenized\\001.txt'\n",
    "d2 = r'C:\\Users\\Vrindavan\\Downloads\\Krithika_Verma_HW1\\tokenized\\001.txt'\n",
    "\n",
    "def read_file(filename): \n",
    "\t\n",
    "\ttry: \n",
    "\t\twith open(filename, 'r') as f: \n",
    "\t\t\tdata = f.read() \n",
    "\t\treturn data \n",
    "\t\n",
    "\texcept IOError: \n",
    "\t\tprint(\"Error opening or reading input file: \", filename) \n",
    "\t\tsys.exit() \n",
    "\n",
    "# splitting the text lines into words \n",
    "# translation table is a global variable \n",
    "# mapping upper case to lower case and \n",
    "# punctuation to spaces \n",
    "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase, \n",
    "\t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase) \n",
    "\t\n",
    "# returns a list of the words \n",
    "# in the file \n",
    "def get_words_from_line_list(text): \n",
    "\t\n",
    "\ttext = text.translate(translation_table) \n",
    "\tword_list = text.split() \n",
    "\t\n",
    "\treturn word_list \n",
    "\n",
    "\n",
    "# counts frequency of each word \n",
    "# returns a dictionary which maps \n",
    "# the words to their frequency. \n",
    "def count_frequency(word_list): \n",
    "\t\n",
    "\tD = {} \n",
    "\t\n",
    "\tfor new_word in word_list: \n",
    "\t\t\n",
    "\t\tif new_word in D: \n",
    "\t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "\t\telse: \n",
    "\t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "\treturn D \n",
    "\n",
    "# returns dictionary of (word, frequency) \n",
    "# pairs from the previous dictionary. \n",
    "def word_frequencies_for_file(filename): \n",
    "\t\n",
    "\tline_list = read_file(filename) \n",
    "\tword_list = get_words_from_line_list(line_list) \n",
    "\tfreq_mapping = count_frequency(word_list) \n",
    "\n",
    "\tprint(\"File\", filename, \":\", ) \n",
    "\tprint(len(line_list), \"lines, \", ) \n",
    "\tprint(len(word_list), \"words, \", ) \n",
    "\tprint(len(freq_mapping), \"distinct words\") \n",
    "\n",
    "\treturn freq_mapping \n",
    "\n",
    "\n",
    "# returns the dot product of two documents \n",
    "def dotProduct(D1, D2): \n",
    "\tSum = 0.0\n",
    "\t\n",
    "\tfor key in D1: \n",
    "\t\t\n",
    "\t\tif key in D2: \n",
    "\t\t\tSum += (D1[key] * D2[key]) \n",
    "\t\t\t\n",
    "\treturn Sum\n",
    "\n",
    "# returns the angle in radians \n",
    "# between document vectors \n",
    "def vector_angle(D1, D2): \n",
    "\tnumerator = dotProduct(D1, D2) \n",
    "\tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2)) \n",
    "\t\n",
    "\treturn math.acos(numerator / denominator) \n",
    "\n",
    "\n",
    "def documentSimilarity(d1, d2): \n",
    "\t\n",
    "# filename_1 = sys.argv[1] \n",
    "# filename_2 = sys.argv[2] \n",
    "\tsorted_word_list_1 = word_frequencies_for_file(d1) \n",
    "\tsorted_word_list_2 = word_frequencies_for_file(d2) \n",
    "\tdistance = vector_angle(sorted_word_list_1, sorted_word_list_2) \n",
    "\t\n",
    "\tprint(\"The distance between the documents is: % 0.6f (radians)\"% distance) \n",
    "\t\n",
    "# Driver code \n",
    "documentSimilarity(d1, d2) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
